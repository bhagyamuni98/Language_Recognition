{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, joblib\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.autograd import grad\n",
    "import cf_matrix\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(17015, 300, 20), y_train:(17015,)\n",
      "X_valid:(4567, 300, 20), y_valid:(4567,)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD #########\n",
    "# choose load file name \n",
    "train_path=\"/home/bhagya/Cross_corpora_data_Bhagya/\"\n",
    "test_path=\"/home/bhagya/Cross_corpora_data_Bhagya/\"\n",
    "\n",
    "f1 = train_path+'X_vad30_original_cms_IIITH.npy'\n",
    "f2 = train_path+'y_vad30_original_cms_IIITH.npy'\n",
    "#f3 = train_path+'X_Kaldi_aug_cms_IIITH.npy'\n",
    "#f4 = train_path+'y_Kaldi_aug_cms_IIITH.npy'\n",
    "f5 = test_path+'X_vad30_original_IIITH_test.npy'\n",
    "f6 = test_path+'y_vad30_original_IIITH_test.npy'\n",
    "\n",
    "X_train = np.load(f1,allow_pickle=True)\n",
    "#X_aug=np.load(f3,allow_pickle=True)\n",
    "X_valid = np.load(f5,allow_pickle=True)\n",
    "y_train = np.load(f2,allow_pickle=True)\n",
    "#y_aug=np.load(f4,allow_pickle=True)\n",
    "y_valid = np.load(f6,allow_pickle=True)\n",
    "\n",
    "# Check that we've recovered the right data\n",
    "print(f'X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "#print(f'X_aug:{X_aug.shape}, y_aug:{y_aug.shape}') \n",
    "print(f'X_valid:{X_valid.shape}, y_valid:{y_valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4], dtype=object), array([3878, 3619, 3442, 3571, 2505]))\n",
      "(array([0, 1, 2, 3, 4], dtype=object), array([ 833, 1077, 1007, 1134,  516]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train,return_counts=True))\n",
    "#print(np.unique(y_aug,return_counts=True))\n",
    "print(np.unique(y_valid,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17015, 300, 20)\n",
      "(array([0, 1, 2, 3, 4], dtype=object), array([3878, 3619, 3442, 3571, 2505]))\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(np.unique(y_train,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECAPA_TDNN(\n",
      "  (layer1): Conv1dReluBn(\n",
      "    (conv): Conv1d(20, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv1dReluBn(\n",
      "      (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Res2Conv1dReluBn(\n",
      "      (convs): ModuleList(\n",
      "        (0-6): 7 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-6): 7 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Conv1dReluBn(\n",
      "      (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): SE_Connect(\n",
      "      (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv1dReluBn(\n",
      "      (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Res2Conv1dReluBn(\n",
      "      (convs): ModuleList(\n",
      "        (0-6): 7 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-6): 7 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Conv1dReluBn(\n",
      "      (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): SE_Connect(\n",
      "      (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv1dReluBn(\n",
      "      (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Res2Conv1dReluBn(\n",
      "      (convs): ModuleList(\n",
      "        (0-6): 7 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-6): 7 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Conv1dReluBn(\n",
      "      (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): SE_Connect(\n",
      "      (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
      "  (bn_conv): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (stats): AttentiveStatsPool(\n",
      "    (linear1): Conv1d(1536, 128, kernel_size=(1,), stride=(1,))\n",
      "    (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (bn_stats): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tail_dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc2): ReluBatchNormTdnnLayer(\n",
      "    (affine): TdnnAffine(3072, 192, context=[0], bias=True, stride=1, pad=True, groups=1, norm_w=False, norm_f=False)\n",
      "    (activation): ReLU(inplace=True)\n",
      "    (batchnorm): BatchNorm1d(192, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (loss): MarginSoftmaxLoss(\n",
      "    (~affine): (input_dim=192, num_targets=5, method=am, double=False, margin=0.2, s=30, t=1.0, feature_normalize=True, mhe_loss=False, mhe_w=0.01, eps=1e-10)\n",
      "    (loss_function): CrossEntropyLoss()\n",
      "  )\n",
      ")\n",
      "torch.Size([2, 192, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhagya/miniconda3/envs/env1/lib/python3.11/site-packages/torch/nn/functional.py:1381: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    }
   ],
   "source": [
    "#ecapa noramal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/bhagya/asv-subtools-master/pytorch/\")\n",
    "import libs.support.utils as utils\n",
    "from libs.nnet import *\n",
    "\n",
    "# refs:\n",
    "# 1.  ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification\n",
    "#           https://arxiv.org/abs/2005.07143\n",
    "# 2.  Unofficial implementation of the ECAPA-TDNN model.\n",
    "#       https://github.com/lawlict/ECAPA-TDNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' Res2Conv1d + BatchNorm1d + ReLU\n",
    "'''\n",
    "class Res2Conv1dReluBn(nn.Module):\n",
    "    '''\n",
    "    inputs_dim == out_channels == channels\n",
    "    '''\n",
    "    def __init__(self, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False, scale=4):\n",
    "        super().__init__()\n",
    "        assert channels % scale == 0, \"{} % {} != 0\".format(channels, scale)\n",
    "        self.scale = scale\n",
    "        self.width = channels // scale\n",
    "        self.nums = scale if scale == 1 else scale - 1\n",
    "\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(nn.Conv1d(self.width, self.width, kernel_size, stride, padding, dilation, bias=bias))\n",
    "            self.bns.append(nn.BatchNorm1d(self.width))\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.bns = nn.ModuleList(self.bns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        spx = torch.split(x, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            # Order: conv -> relu -> bn\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.bns[i](F.relu(sp))\n",
    "            out.append(sp)\n",
    "        if self.scale != 1:\n",
    "            out.append(spx[self.nums])\n",
    "        out = torch.cat(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "''' Conv1d + BatchNorm1d + ReLU\n",
    "'''\n",
    "class Conv1dReluBn(nn.Module):\n",
    "    def __init__(self, inputs_dim, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(inputs_dim, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(F.relu(self.conv(x)))\n",
    "\n",
    "\n",
    "\n",
    "''' The SE connection of 1D case.\n",
    "'''\n",
    "class SE_Connect(nn.Module):\n",
    "    def __init__(self, channels, s=4):\n",
    "        super().__init__()\n",
    "        assert channels % s == 0, \"{} % {} != 0\".format(channesl, s)\n",
    "        self.linear1 = nn.Linear(channels, channels // s)\n",
    "        self.linear2 = nn.Linear(channels // s, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.mean(dim=2)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        out = x * out.unsqueeze(2)\n",
    "        return out\n",
    "\n",
    "#Another implementation of SE_Connect\n",
    "# class SE_Connect(nn.Module):\n",
    "#     def __init__(self, channels, bottleneck=128):\n",
    "#         super(SE_Connect, self).__init__()\n",
    "#         self.se = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.BatchNorm1d(bottleneck),\n",
    "#             nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n",
    "#             nn.Sigmoid(),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = self.se(input)\n",
    "#         return input * x\n",
    "\n",
    "\n",
    "''' SE-Res2Block.\n",
    "    Note: residual connection is implemented in the ECAPA_TDNN model, not here.\n",
    "'''\n",
    "def SE_Res2Block(channels, kernel_size, stride, padding, dilation, scale):\n",
    "    return nn.Sequential(\n",
    "        Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        Res2Conv1dReluBn(channels, kernel_size, stride, padding, dilation, scale=scale),\n",
    "        Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        SE_Connect(channels)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "''' Attentive weighted mean and standard deviation pooling.\n",
    "'''\n",
    "class AttentiveStatsPool(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
    "        self.linear1 = nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1) # equals W and b in the paper\n",
    "        self.linear2 = nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1) # equals V and k in the paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DON'T use ReLU here! In experiments, I find ReLU hard to converge.\n",
    "        alpha = torch.tanh(self.linear1(x))\n",
    "        alpha = torch.softmax(self.linear2(alpha), dim=2)\n",
    "        mean = torch.sum(alpha * x, dim=2)\n",
    "        residuals = torch.sum(alpha * x ** 2, dim=2) - mean ** 2\n",
    "        std = torch.sqrt(residuals.clamp(min=1e-9))\n",
    "        return torch.cat([mean, std], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "''' Implementation of\n",
    "    \"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification\".\n",
    "\n",
    "    Note that we DON'T concatenate the last frame-wise layer with non-weighted mean and standard deviation,\n",
    "    because it brings little improvment but significantly increases model parameters.\n",
    "    As a result, this implementation basically equals the A.2 of Table 2 in the paper.\n",
    "'''\n",
    "class ECAPA_TDNN(TopVirtualNnet):\n",
    "    def init(self, inputs_dim, num_targets, channels=512, embd_dim=192,\n",
    "             aug_dropout=0.5, tail_dropout=0.5, training=True,\n",
    "             extracted_embedding=\"near\", mixup=False, mixup_alpha=1.0,\n",
    "             pooling=\"ecpa-attentive\", pooling_params={}, fc1=False, fc1_params={}, fc2_params={},\n",
    "             margin_loss= True, margin_loss_params={}, use_step=False, step_params={}, transfer_from=\"softmax_loss\" ):\n",
    "\n",
    "\n",
    "        default_pooling_params = {\n",
    "            \"num_head\":1,\n",
    "            \"hidden_size\":64,\n",
    "            \"share\":True,\n",
    "            \"affine_layers\":1,\n",
    "            \"context\":[0],\n",
    "            \"stddev\":True,\n",
    "            \"temperature\":False,\n",
    "            \"fixed\":True\n",
    "        }\n",
    "\n",
    "        default_fc_params = {\n",
    "            \"nonlinearity\":'relu', \"nonlinearity_params\":{\"inplace\":True},\n",
    "            \"bn-relu\":False,\n",
    "            \"bn\":True,\n",
    "            \"bn_params\":{\"momentum\":0.5, \"affine\":True, \"track_running_stats\":True}\n",
    "            }\n",
    "\n",
    "\n",
    "        default_margin_loss_params = {\n",
    "            \"method\":\"am\", \"m\":0.2,\n",
    "            \"feature_normalize\":True, \"s\":30,\n",
    "            \"double\":False,\n",
    "            \"mhe_loss\":False, \"mhe_w\":0.01,\n",
    "            \"inter_loss\":0.,\n",
    "            \"ring_loss\":0.,\n",
    "            \"curricular\":False}\n",
    "\n",
    "        default_step_params = {\n",
    "            \"T\":None,\n",
    "            \"m\":False, \"lambda_0\":0, \"lambda_b\":1000, \"alpha\":5, \"gamma\":1e-4,\n",
    "            \"s\":False, \"s_tuple\":(30, 12), \"s_list\":None,\n",
    "            \"t\":False, \"t_tuple\":(0.5, 1.2),\n",
    "            \"p\":False, \"p_tuple\":(0.5, 0.1)\n",
    "        }\n",
    "\n",
    "        self.use_step = use_step\n",
    "        self.step_params = step_params\n",
    "        self.extracted_embedding = extracted_embedding\n",
    "\n",
    "        pooling_params = utils.assign_params_dict(default_pooling_params, pooling_params)\n",
    "        fc1_params = utils.assign_params_dict(default_fc_params, fc1_params)\n",
    "        fc2_params = utils.assign_params_dict(default_fc_params, fc2_params)\n",
    "        margin_loss_params = utils.assign_params_dict(default_margin_loss_params, margin_loss_params)\n",
    "        step_params = utils.assign_params_dict(default_step_params, step_params)\n",
    "\n",
    "\n",
    "        #self.mixup = Mixup(alpha=mixup_alpha) if mixup else None\n",
    "        #self.pcmvn = AdaptivePCMN(20)\n",
    "        self.layer1 = Conv1dReluBn(inputs_dim, channels, kernel_size=5, padding=2)\n",
    "        self.layer2 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=2, dilation=2, scale=8)\n",
    "        self.layer3 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=3, dilation=3, scale=8)\n",
    "        self.layer4 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=4, dilation=4, scale=8)\n",
    "        cat_channels = channels * 3\n",
    "        self.conv = nn.Conv1d(cat_channels, cat_channels, kernel_size=1)\n",
    "        self.bn_conv = nn.BatchNorm1d(cat_channels)\n",
    "\n",
    "        # Pooling\n",
    "        stddev = pooling_params.pop(\"stddev\")\n",
    "        if pooling == \"attentive\":\n",
    "            self.stats = AttentiveStatisticsPooling(cat_channels, hidden_size=pooling_params[\"hidden_size\"],context=pooling_params[\"context\"], stddev=stddev)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"ecpa-attentive\":\n",
    "            self.stats = AttentiveStatsPool(cat_channels,128)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"multi-head\":\n",
    "            self.stats = MultiHeadAttentionPooling(cat_channels, stddev=stddev, **pooling_params)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"global-multi\":\n",
    "            self.stats = GlobalMultiHeadAttentionPooling(cat_channels,stddev=stddev, **pooling_params)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2* pooling_params[\"num_head\"])\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2* pooling_params[\"num_head\"], embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"multi-resolution\":\n",
    "            self.stats = MultiResolutionMultiHeadAttentionPooling(cat_channels, **pooling_params)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2* pooling_params[\"num_head\"])\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2* pooling_params[\"num_head\"], embd_dim, **fc1_params) if fc1 else None\n",
    "\n",
    "        else:\n",
    "            self.stats = StatisticsPooling(cat_channels, stddev=stddev)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "\n",
    "        self.tail_dropout = torch.nn.Dropout2d(p=tail_dropout) if tail_dropout > 0 else None\n",
    "\n",
    "        if fc1:\n",
    "            fc2_in_dim = embd_dim\n",
    "        else:\n",
    "            fc2_in_dim = cat_channels * 2\n",
    "        self.fc2 = ReluBatchNormTdnnLayer(fc2_in_dim, embd_dim, **fc2_params)\n",
    "        self.tail_dropout = torch.nn.Dropout2d(p=tail_dropout) if tail_dropout > 0 else None\n",
    "\n",
    "         # Loss\n",
    "        # Do not need when extracting embedding.\n",
    "        if training :\n",
    "            if margin_loss:\n",
    "                self.loss = MarginSoftmaxLoss(embd_dim, num_targets, **margin_loss_params)\n",
    "            else:\n",
    "                self.loss = SoftmaxLoss(embd_dim, num_targets)\n",
    "                # self.loss = AngleLoss(embd_dim,num_targets)\n",
    "            self.wrapper_loss = MixupLoss(self.loss, self.mixup) if mixup else None\n",
    "            # An example to using transform-learning without initializing loss.affine parameters\n",
    "            self.transform_keys = [\"layer2\",\"layer3\",\"layer4\",\"conv\",\"stats\",\"fc1\",\"fc2\"]\n",
    "\n",
    "            if margin_loss and transfer_from == \"softmax_loss\":\n",
    "                # For softmax_loss to am_softmax_loss\n",
    "                self.rename_transform_keys = {\"loss.affine.weight\":\"loss.weight\"}\n",
    "\n",
    "    @utils.for_device_free\n",
    "    def forward(self, x):\n",
    "        #x = self.mixup(x)\n",
    "        #x = self.pcmvn(x)\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1) + out1\n",
    "        out3 = self.layer3(out1 + out2) + out1 + out2\n",
    "        out4 = self.layer4(out1 + out2 + out3) + out1 + out2 + out3\n",
    "        out = torch.cat([out2, out3, out4], dim=1)\n",
    "        out = self.bn_conv(F.relu(self.conv(out)))\n",
    "        x = self.bn_stats(self.stats(out))\n",
    "        if len(x.shape) !=3:\n",
    "            x = x.unsqueeze(dim=2)\n",
    "        x = self.auto(self.fc1, x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.auto(self.tail_dropout, x)\n",
    "        return x\n",
    "\n",
    "    @utils.for_device_free\n",
    "    def get_loss(self, inputs, targets):\n",
    "        \"\"\"Should call get_loss() after forward() with using Xvector model function.\n",
    "        e.g.:\n",
    "            m=Xvector(20,10)\n",
    "            loss=m.get_loss(m(inputs),targets)\n",
    "\n",
    "        model.get_loss [custom] -> loss.forward [custom]\n",
    "          |\n",
    "          v\n",
    "        model.get_accuracy [custom] -> loss.get_accuracy [custom] -> loss.compute_accuracy [static] -> loss.predict [static]\n",
    "        \"\"\"\n",
    "        if self.wrapper_loss is not None:\n",
    "            return self.wrapper_loss(inputs, targets)\n",
    "        else:\n",
    "            return self.loss(inputs, targets)\n",
    "\n",
    "    @utils.for_device_free\n",
    "    def get_accuracy(self, targets):\n",
    "        \"\"\"Should call get_accuracy() after get_loss().\n",
    "        @return: return accuracy\n",
    "        \"\"\"\n",
    "        if self.wrapper_loss is not None:\n",
    "            return self.wrapper_loss.get_accuracy(targets)\n",
    "        else:\n",
    "            return self.loss.get_accuracy(targets)\n",
    "\n",
    "    @for_extract_embedding(maxChunk=10000, isMatrix=True)\n",
    "    def extract_embedding(self, inputs):\n",
    "        out1 = self.layer1(inputs)\n",
    "        out2 = self.layer2(out1) + out1\n",
    "        out3 = self.layer3(out1 + out2) + out1 + out2\n",
    "        out4 = self.layer4(out1 + out2 + out3) + out1 + out2 + out3\n",
    "        out = torch.cat([out2, out3, out4], dim=1)\n",
    "        out = self.bn_conv(F.relu(self.conv(out)))\n",
    "        x = self.bn_stats(self.stats(out))\n",
    "        if len(x.shape) !=3:\n",
    "            x = x.unsqueeze(dim=2)\n",
    "        if self.extracted_embedding == \"far\":\n",
    "            assert self.fc1 is not None\n",
    "            xvector = self.fc1.affine(x)\n",
    "        elif self.extracted_embedding == \"near_affine\":\n",
    "            x = self.auto(self.fc1, x)\n",
    "            xvector = self.fc2.affine(x)\n",
    "        elif self.extracted_embedding == \"near\":\n",
    "            x = self.auto(self.fc1, x)\n",
    "            xvector = self.fc2(x)\n",
    "        else:\n",
    "            raise TypeError(\"Expected far or near position, but got {}\".format(self.extracted_embedding))\n",
    "        return xvector\n",
    "    def get_warmR_T(T_0, T_mult, epoch):\n",
    "        n = int(math.log(max(0.05, (epoch / T_0 * (T_mult - 1) + 1)), T_mult))\n",
    "        T_cur = epoch - T_0 * (T_mult ** n - 1) / (T_mult - 1)\n",
    "        T_i = T_0 * T_mult ** (n)\n",
    "        return T_cur, T_i\n",
    "\n",
    "    def compute_decay_value(self, start, end, T_cur, T_i):\n",
    "        # Linear decay in every cycle time.\n",
    "        return start - (start - end)/(T_i-1) * (T_cur%T_i)\n",
    "\n",
    "    def step(self, epoch, this_iter, epoch_batchs):\n",
    "        # Heated up for t and s.\n",
    "        # Decay for margin and dropout p.\n",
    "        if self.use_step:\n",
    "            if self.step_params[\"m\"]:\n",
    "                current_postion = epoch*epoch_batchs + this_iter\n",
    "                lambda_factor = max(self.step_params[\"lambda_0\"],\n",
    "                                 self.step_params[\"lambda_b\"]*(1+self.step_params[\"gamma\"]*current_postion)**(-self.step_params[\"alpha\"]))\n",
    "                self.loss.step(lambda_factor)\n",
    "\n",
    "            if self.step_params[\"T\"] is not None and (self.step_params[\"t\"] or self.step_params[\"p\"]):\n",
    "                T_cur, T_i = get_warmR_T(*self.step_params[\"T\"], epoch)\n",
    "                T_cur = T_cur*epoch_batchs + this_iter\n",
    "                T_i = T_i * epoch_batchs\n",
    "\n",
    "            if self.step_params[\"t\"]:\n",
    "                self.loss.t = self.compute_decay_value(*self.step_params[\"t_tuple\"], T_cur, T_i)\n",
    "\n",
    "            if self.step_params[\"p\"]:\n",
    "                self.aug_dropout.p = self.compute_decay_value(*self.step_params[\"p_tuple\"], T_cur, T_i)\n",
    "\n",
    "            if self.step_params[\"s\"]:\n",
    "                self.loss.s = self.step_params[\"s_tuple\"][self.step_params[\"s_list\"][epoch]]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Input size: batch_size * seq_len * feat_dim\n",
    "    x = torch.zeros(2, 20, 300)\n",
    "    model = ECAPA_TDNN(inputs_dim=20,num_targets=5, channels=512, embd_dim=192)\n",
    "    out = model(x)\n",
    "    print(model)\n",
    "    print(out.shape)    # should be [2, 192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_class_weight(y,tot_class):\n",
    "    classes=np.unique(y,return_counts=True)\n",
    "    total_samples=np.sum(np.array(classes[1]))\n",
    "    class_weights=np.empty(tot_class)\n",
    "    for i,j in enumerate(classes[1]):\n",
    "        print(i,'--->',j)\n",
    "        class_weights[i]=total_samples/(tot_class*j)\n",
    "    print(class_weights)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---> 3878\n",
      "1 ---> 3619\n",
      "2 ---> 3442\n",
      "3 ---> 3571\n",
      "4 ---> 2505\n",
      "[0.87751418 0.940315   0.98866938 0.95295435 1.35848303]\n",
      "[0.64595152 0.69218016 0.72777455 0.70148418 1.        ]\n"
     ]
    }
   ],
   "source": [
    "class_weights_train=imbalance_class_weight(y_train,5)\n",
    "class_weights_train2=class_weights_train/np.max(class_weights_train)\n",
    "print(class_weights_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhagya/miniconda3/envs/env1/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#deice=torch.device(\"cpu\")\n",
    "model = ECAPA_TDNN(20, 5).to(device)\n",
    "class_weights_train = torch.FloatTensor(class_weights_train2).to(device)\n",
    "loss_fun = nn.CrossEntropyLoss(weight=class_weights_train)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)#, weight_decay=0.0000001, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=1e-8, eps=1e-08, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xv_data_manage(X_test):\n",
    "     aa=X_test.transpose((0,2,1))\n",
    "     print(X_test.shape, '-->', aa.shape)\n",
    "     return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17015, 300, 20) --> (17015, 20, 300)\n",
      "(4567, 300, 20) --> (4567, 20, 300)\n"
     ]
    }
   ],
   "source": [
    " X_train=xv_data_manage(X_train)\n",
    " X_valid=xv_data_manage(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(list(zip(X_train,y_train)), shuffle=True, batch_size=300,drop_last=False)\n",
    "testloader1 = torch.utils.data.DataLoader(list(zip(X_valid,y_valid)), shuffle=False, batch_size=32,drop_last=False)\n",
    "#testloader2 = torch.utils.data.DataLoader(list(zip(X_test,y_test)), shuffle=True, batch_size=32,drop_last=False)\n",
    "print(len(trainloader))\n",
    "print(len(testloader1))\n",
    "#print(len(testloader2))\n",
    "#print(len(testloader3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 176.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m---> 97\u001b[0m         mean_acc\u001b[38;5;241m=\u001b[39mtrain(trainloader,epoch)\n\u001b[1;32m     98\u001b[0m         val_loss\u001b[38;5;241m=\u001b[39mvalidation(testloader1,epoch,mean_acc)\n\u001b[1;32m     99\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader_train, epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m features\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m pred_logits \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_loss(pred_logits,labels)\n\u001b[1;32m     29\u001b[0m posterior\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_posterior()\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/asv-subtools-master/pytorch/libs/support/utils.py:154\u001b[0m, in \u001b[0;36mfor_device_free.<locals>.wrapper\u001b[0;34m(self, *tensor_sets)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m get_tensors(tensor_sets):\n\u001b[1;32m    152\u001b[0m     transformed\u001b[38;5;241m.\u001b[39mappend(to_device(\u001b[38;5;28mself\u001b[39m, tensor))\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtransformed)\n",
      "Cell \u001b[0;32mIn[5], line 270\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mfor_device_free\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m#x = self.mixup(x)\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m#x = self.pcmvn(x)\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    271\u001b[0m     out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out1) \u001b[38;5;241m+\u001b[39m out1\n\u001b[1;32m    272\u001b[0m     out3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(out1 \u001b[38;5;241m+\u001b[39m out2) \u001b[38;5;241m+\u001b[39m out1 \u001b[38;5;241m+\u001b[39m out2\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mConv1dReluBn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)))\n",
      "File \u001b[0;32m~/miniconda3/envs/env1/lib/python3.11/site-packages/torch/nn/functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 176.00 MiB. GPU "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#For LR scheduler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dest_path=\"/home/bhagya/Data_Bhagya_internship_wav2vec_xlsr_IIITH_23L/\"\n",
    "os.makedirs(dest_path,exist_ok=True)\n",
    "#Languages=[\"Assamese\",\"Bengali\",\"Bodo\",\"Dogri\",\"English\",\"Gujarati\",\"Hindi\",\"Kannada\",\"Kashmiri\",\"Konkani\",\"Maithili\",\"Manipuri\",\"Malayalam\",\"Marathi\",\"Nepali\",\"Odia\",\"Punjabi\",\"Sanskrit\",\"Santali\",\"Sindhi\",\"Tamil\",\"Telugu\",\"Urdu\"]\n",
    "Languages=[\"Bengali\",\"Hindi\",\"Punjabi\",\"Tamil\",\"Urdu\"]\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cf_matrix\n",
    "def train(dataloader_train,epoch):\n",
    "    train_loss_list=[]\n",
    "    train_acc_list=[]\n",
    "    full_preds=[]\n",
    "    full_gts=[]\n",
    "\n",
    "    model.train()\n",
    "    for i_batch, data in enumerate(dataloader_train, 0):\n",
    "        features, labels = data\n",
    "        #features=torch.transpose(features,0,2,1)\n",
    "        #print(features.shape)\n",
    "        #print(labels.shape)\n",
    "        #features=minmax(features)\n",
    "        features, labels = features.to(device).float(),labels.to(device).long()\n",
    "        features.requires_grad = True\n",
    "        optimizer.zero_grad()\n",
    "        pred_logits = model(features)\n",
    "        loss=model.get_loss(pred_logits,labels)\n",
    "        posterior=model.get_posterior()\n",
    "        #print(posterior.shape)\n",
    "        #loss = loss_fun(pred_logits,labels)\n",
    "\n",
    "        #print(pred_logits.shape)\n",
    "        #LOSS=[]\n",
    "        #for ii in range(features.shape[1]):\n",
    "        #    loss = loss_fun(features[:,ii,:],labels)\n",
    "        #    LOSS.append(loss)\n",
    "        #loss=torch.mean(torch.tensor(LOSS))\n",
    "        #loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predictions = np.argmax(posterior.detach().cpu().numpy(),axis=1)\n",
    "        if i_batch%50==0:\n",
    "            #print(final_pred)\n",
    "            acc = accuracy_score(predictions,labels.cpu())\n",
    "            print(f\"Epoch: {epoch}, Batch: {i_batch}, Loss: {loss}, Accuracy:{acc*100}%\")\n",
    "        train_loss_list.append(loss.item())\n",
    "        for pred in predictions:\n",
    "            full_preds.append(pred)\n",
    "        for lab in labels.detach().cpu().numpy():\n",
    "            full_gts.append(lab)\n",
    "    mean_acc = accuracy_score(full_gts,full_preds)\n",
    "    mean_loss = np.mean(np.asarray(train_loss_list))\n",
    "    return mean_acc\n",
    "    print('Total training loss {} and training Accuracy {} after {} epochs'.format(mean_loss,mean_acc,epoch))\n",
    "def validation(dataloader_val,epoch,train_accuracy):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_list=[]\n",
    "        full_preds=[]\n",
    "        full_gts=[]\n",
    "        for i_batch, data in enumerate(dataloader_val, 0):\n",
    "            features, labels = data\n",
    "            #features=torch.transpose(features,0,2,1)\n",
    "            features, labels = features.to(device).float(),labels.to(device).long()\n",
    "            pred_logits = model(features)\n",
    "            #print(features.shape)\n",
    "            #print(labels.shape)\n",
    "            loss=model.get_loss(pred_logits,labels)\n",
    "            posterior=model.get_posterior()\n",
    "            #loss = loss_fun(pred_logits,labels)\n",
    "            val_loss_list.append(loss.item())\n",
    "            predictions = np.argmax(posterior.detach().cpu().numpy(),axis=1)\n",
    "            if i_batch%50==0:\n",
    "                #print(final_pred)\n",
    "                acc = accuracy_score(predictions,labels.cpu())\n",
    "                print(f\"#####TEST--in-domain:::::Epoch: {epoch}, Batch: {i_batch}, Loss: {loss}, Accuracy:{acc*100}%\")\n",
    "            for pred in predictions:\n",
    "                full_preds.append(pred)\n",
    "            for lab in labels.detach().cpu().numpy():\n",
    "                full_gts.append(lab)\n",
    "\n",
    "        mean_acc2 = accuracy_score(full_gts,full_preds)\n",
    "        mean_loss2 = np.mean(np.asarray(val_loss_list))\n",
    "        cm=confusion_matrix(full_gts,full_preds)\n",
    "        cf_matrix.make_confusion_matrix(cm, percent=False, figsize=(8,6), categories=Languages, cbar=True, title=\"Confusion_matrix: Validation\")\n",
    "        plt.show()\n",
    "        print(':::::Total vlidation loss {}, Training accuracy {}, Validation accuracy {} after {} epochs'.format(mean_loss2,train_accuracy, mean_acc2,epoch))\n",
    "        print(\"#!#!#!#!!!##!##!!####!#####!###!#!##!##!##!#####!#############\")\n",
    "        model_save_path = os.path.join(dest_path,'IIITH_merged_original_Bhagya_ecapa_cms_'+str(\"{:.3f}\".format(train_accuracy*100))+\"_\"+str(\"{:.3f}\".format(mean_acc2*100))+\"_\"+str(\"{:.4f}\".format(mean_loss2)))\n",
    "        state_dict = {'model': model.state_dict(),'optimizer': optimizer.state_dict(),'epoch': epoch}\n",
    "        torch.save(state_dict, model_save_path)\n",
    "        return loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(50):\n",
    "        mean_acc=train(trainloader,epoch)\n",
    "        val_loss=validation(testloader1,epoch,mean_acc)\n",
    "        scheduler.step(val_loss)\n",
    "        #testing(testloader2,epoch,mean_acc)\n",
    "        #testing(testloader3,epoch,mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
