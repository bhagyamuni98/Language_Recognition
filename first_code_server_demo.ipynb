{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, joblib\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(24540, 300, 1024), y_train:(24540,)\n",
      "X_valid:(6818, 300, 1024), y_valid:(6818,)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD #########\n",
    "# choose load file name\n",
    "data_path=\"/home/bhagya/Data_Bhagya_internship_wav2vec_xlsr_IIITH_23L\"\n",
    "f1 = data_path+'/X_train_original_VAD30_wav2vec2_xlsr_small_last16_cms.npy'\n",
    "f2 = data_path+'/y1_train_original_VAD30_wav2vec2_xlsr_small_last16_cms.npy'\n",
    "#f3 = 'X_train_AUG-10-15-25-35-40-45-50_IIITH_23L_MFCC_cms_new.npy'\n",
    "#f4 = 'y_train_AUG-10-15-25-35-40-45-50_IIITH_23L_MFCC_cms_new.npy'\n",
    "f5 = data_path+'/X_test_original_VAD30_wav2vec2_xlsr_small_last16_cms.npy'\n",
    "f6 = data_path+'/y1_test_original_VAD30_wav2vec2_xlsr_small_last16_cms.npy'\n",
    "\n",
    "X_train = np.load(f1)\n",
    "#X_aug=np.load(f3,allow_pickle=True)\n",
    "X_valid = np.load(f5)\n",
    "y_train = np.load(f2,allow_pickle=True)\n",
    "#y_aug=np.load(f4,allow_pickle=True)\n",
    "y_valid = np.load(f6,allow_pickle=True)\n",
    "\n",
    "# Check that we've recovered the right data\n",
    "print(f'X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "#print(f'X_aug:{X_aug.shape}, y_aug:{y_aug.shape}')\n",
    "print(f'X_valid:{X_valid.shape}, y_valid:{y_valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_train,return_counts=True))\n",
    "#print(np.unique(y_aug,return_counts=True))\n",
    "print(np.unique(y_valid,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./miniconda3/envs/env1/lib/python3.11/site-packages (24.0)\n",
      "Requirement already satisfied: torchsummary in ./miniconda3/envs/env1/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: libs in ./miniconda3/envs/env1/lib/python3.11/site-packages (0.0.10)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement libs.support (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for libs.support\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement libs.nnet (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for libs.nnet\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torchsummary\n",
    "!pip install libs\n",
    "!pip install libs.support\n",
    "!pip install libs.nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TopVirtualNnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 147\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([mean, std], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m''' Implementation of\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification\".\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    As a result, this implementation basically equals the A.2 of Table 2 in the paper.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mECAPA_TDNN\u001b[39;00m(TopVirtualNnet):\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs_dim, num_targets, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, embd_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m192\u001b[39m,\n\u001b[1;32m    149\u001b[0m              aug_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, tail_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m              extracted_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnear\u001b[39m\u001b[38;5;124m\"\u001b[39m, mixup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mixup_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    151\u001b[0m              pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecpa-attentive\u001b[39m\u001b[38;5;124m\"\u001b[39m, pooling_params\u001b[38;5;241m=\u001b[39m{}, fc1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, fc1_params\u001b[38;5;241m=\u001b[39m{}, fc2_params\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    152\u001b[0m              margin_loss\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, margin_loss_params\u001b[38;5;241m=\u001b[39m{}, use_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, step_params\u001b[38;5;241m=\u001b[39m{}, transfer_from\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m ):\n\u001b[1;32m    155\u001b[0m         default_pooling_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_head\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TopVirtualNnet' is not defined"
     ]
    }
   ],
   "source": [
    "#ecapa noramal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/drive/MyDrive/MTP/asv-subtools-master/pytorch/\")\n",
    "import libs.support.utils as utils\n",
    "from libs.nnet import *\n",
    "\n",
    "# refs:\n",
    "# 1.  ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification\n",
    "#           https://arxiv.org/abs/2005.07143\n",
    "# 2.  Unofficial implementation of the ECAPA-TDNN model.\n",
    "#       https://github.com/lawlict/ECAPA-TDNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' Res2Conv1d + BatchNorm1d + ReLU\n",
    "'''\n",
    "class Res2Conv1dReluBn(nn.Module):\n",
    "    '''\n",
    "    inputs_dim == out_channels == channels\n",
    "    '''\n",
    "    def __init__(self, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False, scale=4):\n",
    "        super().__init__()\n",
    "        assert channels % scale == 0, \"{} % {} != 0\".format(channels, scale)\n",
    "        self.scale = scale\n",
    "        self.width = channels // scale\n",
    "        self.nums = scale if scale == 1 else scale - 1\n",
    "\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(nn.Conv1d(self.width, self.width, kernel_size, stride, padding, dilation, bias=bias))\n",
    "            self.bns.append(nn.BatchNorm1d(self.width))\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.bns = nn.ModuleList(self.bns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        spx = torch.split(x, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            # Order: conv -> relu -> bn\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.bns[i](F.relu(sp))\n",
    "            out.append(sp)\n",
    "        if self.scale != 1:\n",
    "            out.append(spx[self.nums])\n",
    "        out = torch.cat(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "''' Conv1d + BatchNorm1d + ReLU\n",
    "'''\n",
    "class Conv1dReluBn(nn.Module):\n",
    "    def __init__(self, inputs_dim, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(inputs_dim, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(F.relu(self.conv(x)))\n",
    "\n",
    "\n",
    "\n",
    "''' The SE connection of 1D case.\n",
    "'''\n",
    "class SE_Connect(nn.Module):\n",
    "    def __init__(self, channels, s=4):\n",
    "        super().__init__()\n",
    "        assert channels % s == 0, \"{} % {} != 0\".format(channesl, s)\n",
    "        self.linear1 = nn.Linear(channels, channels // s)\n",
    "        self.linear2 = nn.Linear(channels // s, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.mean(dim=2)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        out = x * out.unsqueeze(2)\n",
    "        return out\n",
    "\n",
    "#Another implementation of SE_Connect\n",
    "# class SE_Connect(nn.Module):\n",
    "#     def __init__(self, channels, bottleneck=128):\n",
    "#         super(SE_Connect, self).__init__()\n",
    "#         self.se = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.BatchNorm1d(bottleneck),\n",
    "#             nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n",
    "#             nn.Sigmoid(),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = self.se(input)\n",
    "#         return input * x\n",
    "\n",
    "\n",
    "''' SE-Res2Block.\n",
    "    Note: residual connection is implemented in the ECAPA_TDNN model, not here.\n",
    "'''\n",
    "def SE_Res2Block(channels, kernel_size, stride, padding, dilation, scale):\n",
    "    return nn.Sequential(\n",
    "        Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        Res2Conv1dReluBn(channels, kernel_size, stride, padding, dilation, scale=scale),\n",
    "        Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        SE_Connect(channels)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "''' Attentive weighted mean and standard deviation pooling.\n",
    "'''\n",
    "class AttentiveStatsPool(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
    "        self.linear1 = nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1) # equals W and b in the paper\n",
    "        self.linear2 = nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1) # equals V and k in the paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DON'T use ReLU here! In experiments, I find ReLU hard to converge.\n",
    "        alpha = torch.tanh(self.linear1(x))\n",
    "        alpha = torch.softmax(self.linear2(alpha), dim=2)\n",
    "        mean = torch.sum(alpha * x, dim=2)\n",
    "        residuals = torch.sum(alpha * x ** 2, dim=2) - mean ** 2\n",
    "        std = torch.sqrt(residuals.clamp(min=1e-9))\n",
    "        return torch.cat([mean, std], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "''' Implementation of\n",
    "    \"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification\".\n",
    "\n",
    "    Note that we DON'T concatenate the last frame-wise layer with non-weighted mean and standard deviation,\n",
    "    because it brings little improvment but significantly increases model parameters.\n",
    "    As a result, this implementation basically equals the A.2 of Table 2 in the paper.\n",
    "'''\n",
    "class ECAPA_TDNN(TopVirtualNnet):\n",
    "    def init(self, inputs_dim, num_targets, channels=512, embd_dim=192,\n",
    "             aug_dropout=0.5, tail_dropout=0.5, training=True,\n",
    "             extracted_embedding=\"near\", mixup=False, mixup_alpha=1.0,\n",
    "             pooling=\"ecpa-attentive\", pooling_params={}, fc1=False, fc1_params={}, fc2_params={},\n",
    "             margin_loss= True, margin_loss_params={}, use_step=False, step_params={}, transfer_from=\"softmax_loss\" ):\n",
    "\n",
    "\n",
    "        default_pooling_params = {\n",
    "            \"num_head\":1,\n",
    "            \"hidden_size\":64,\n",
    "            \"share\":True,\n",
    "            \"affine_layers\":1,\n",
    "            \"context\":[0],\n",
    "            \"stddev\":True,\n",
    "            \"temperature\":False,\n",
    "            \"fixed\":True\n",
    "        }\n",
    "\n",
    "        default_fc_params = {\n",
    "            \"nonlinearity\":'relu', \"nonlinearity_params\":{\"inplace\":True},\n",
    "            \"bn-relu\":False,\n",
    "            \"bn\":True,\n",
    "            \"bn_params\":{\"momentum\":0.5, \"affine\":True, \"track_running_stats\":True}\n",
    "            }\n",
    "\n",
    "\n",
    "        default_margin_loss_params = {\n",
    "            \"method\":\"am\", \"m\":0.2,\n",
    "            \"feature_normalize\":True, \"s\":30,\n",
    "            \"double\":False,\n",
    "            \"mhe_loss\":False, \"mhe_w\":0.01,\n",
    "            \"inter_loss\":0.,\n",
    "            \"ring_loss\":0.,\n",
    "            \"curricular\":False}\n",
    "\n",
    "        default_step_params = {\n",
    "            \"T\":None,\n",
    "            \"m\":False, \"lambda_0\":0, \"lambda_b\":1000, \"alpha\":5, \"gamma\":1e-4,\n",
    "            \"s\":False, \"s_tuple\":(30, 12), \"s_list\":None,\n",
    "            \"t\":False, \"t_tuple\":(0.5, 1.2),\n",
    "            \"p\":False, \"p_tuple\":(0.5, 0.1)\n",
    "        }\n",
    "\n",
    "        self.use_step = use_step\n",
    "        self.step_params = step_params\n",
    "        self.extracted_embedding = extracted_embedding\n",
    "\n",
    "        pooling_params = utils.assign_params_dict(default_pooling_params, pooling_params)\n",
    "        fc1_params = utils.assign_params_dict(default_fc_params, fc1_params)\n",
    "        fc2_params = utils.assign_params_dict(default_fc_params, fc2_params)\n",
    "        margin_loss_params = utils.assign_params_dict(default_margin_loss_params, margin_loss_params)\n",
    "        step_params = utils.assign_params_dict(default_step_params, step_params)\n",
    "\n",
    "\n",
    "        #self.mixup = Mixup(alpha=mixup_alpha) if mixup else None\n",
    "        #self.pcmvn = AdaptivePCMN(20)\n",
    "        self.layer1 = Conv1dReluBn(inputs_dim, channels, kernel_size=5, padding=2)\n",
    "        self.layer2 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=2, dilation=2, scale=8)\n",
    "        self.layer3 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=3, dilation=3, scale=8)\n",
    "        self.layer4 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=4, dilation=4, scale=8)\n",
    "        cat_channels = channels * 3\n",
    "        self.conv = nn.Conv1d(cat_channels, cat_channels, kernel_size=1)\n",
    "        self.bn_conv = nn.BatchNorm1d(cat_channels)\n",
    "\n",
    "        # Pooling\n",
    "        stddev = pooling_params.pop(\"stddev\")\n",
    "        if pooling == \"attentive\":\n",
    "            self.stats = AttentiveStatisticsPooling(cat_channels, hidden_size=pooling_params[\"hidden_size\"],context=pooling_params[\"context\"], stddev=stddev)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"ecpa-attentive\":\n",
    "            self.stats = AttentiveStatsPool(cat_channels,128)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"multi-head\":\n",
    "            self.stats = MultiHeadAttentionPooling(cat_channels, stddev=stddev, **pooling_params)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"global-multi\":\n",
    "            self.stats = GlobalMultiHeadAttentionPooling(cat_channels,stddev=stddev, **pooling_params)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2* pooling_params[\"num_head\"])\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2* pooling_params[\"num_head\"], embd_dim, **fc1_params) if fc1 else None\n",
    "        elif pooling == \"multi-resolution\":\n",
    "            self.stats = MultiResolutionMultiHeadAttentionPooling(cat_channels, **pooling_params)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2* pooling_params[\"num_head\"])\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2* pooling_params[\"num_head\"], embd_dim, **fc1_params) if fc1 else None\n",
    "\n",
    "        else:\n",
    "            self.stats = StatisticsPooling(cat_channels, stddev=stddev)\n",
    "            self.bn_stats = nn.BatchNorm1d(cat_channels * 2)\n",
    "            self.fc1 = ReluBatchNormTdnnLayer(cat_channels * 2, embd_dim, **fc1_params) if fc1 else None\n",
    "\n",
    "        self.tail_dropout = torch.nn.Dropout2d(p=tail_dropout) if tail_dropout > 0 else None\n",
    "\n",
    "        if fc1:\n",
    "            fc2_in_dim = embd_dim\n",
    "        else:\n",
    "            fc2_in_dim = cat_channels * 2\n",
    "        self.fc2 = ReluBatchNormTdnnLayer(fc2_in_dim, embd_dim, **fc2_params)\n",
    "        self.tail_dropout = torch.nn.Dropout2d(p=tail_dropout) if tail_dropout > 0 else None\n",
    "\n",
    "         # Loss\n",
    "        # Do not need when extracting embedding.\n",
    "        if training :\n",
    "            if margin_loss:\n",
    "                self.loss = MarginSoftmaxLoss(embd_dim, num_targets, **margin_loss_params)\n",
    "            else:\n",
    "                self.loss = SoftmaxLoss(embd_dim, num_targets)\n",
    "                # self.loss = AngleLoss(embd_dim,num_targets)\n",
    "            self.wrapper_loss = MixupLoss(self.loss, self.mixup) if mixup else None\n",
    "            # An example to using transform-learning without initializing loss.affine parameters\n",
    "            self.transform_keys = [\"layer2\",\"layer3\",\"layer4\",\"conv\",\"stats\",\"fc1\",\"fc2\"]\n",
    "\n",
    "            if margin_loss and transfer_from == \"softmax_loss\":\n",
    "                # For softmax_loss to am_softmax_loss\n",
    "                self.rename_transform_keys = {\"loss.affine.weight\":\"loss.weight\"}\n",
    "\n",
    "    @utils.for_device_free\n",
    "    def forward(self, x):\n",
    "        #x = self.mixup(x)\n",
    "        #x = self.pcmvn(x)\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1) + out1\n",
    "        out3 = self.layer3(out1 + out2) + out1 + out2\n",
    "        out4 = self.layer4(out1 + out2 + out3) + out1 + out2 + out3\n",
    "        out = torch.cat([out2, out3, out4], dim=1)\n",
    "        out = self.bn_conv(F.relu(self.conv(out)))\n",
    "        x = self.bn_stats(self.stats(out))\n",
    "        if len(x.shape) !=3:\n",
    "            x = x.unsqueeze(dim=2)\n",
    "        x = self.auto(self.fc1, x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.auto(self.tail_dropout, x)\n",
    "        return x\n",
    "\n",
    "    @utils.for_device_free\n",
    "    def get_loss(self, inputs, targets):\n",
    "        \"\"\"Should call get_loss() after forward() with using Xvector model function.\n",
    "        e.g.:\n",
    "            m=Xvector(20,10)\n",
    "            loss=m.get_loss(m(inputs),targets)\n",
    "\n",
    "        model.get_loss [custom] -> loss.forward [custom]\n",
    "          |\n",
    "          v\n",
    "        model.get_accuracy [custom] -> loss.get_accuracy [custom] -> loss.compute_accuracy [static] -> loss.predict [static]\n",
    "        \"\"\"\n",
    "        if self.wrapper_loss is not None:\n",
    "            return self.wrapper_loss(inputs, targets)\n",
    "        else:\n",
    "            return self.loss(inputs, targets)\n",
    "\n",
    "    @utils.for_device_free\n",
    "    def get_accuracy(self, targets):\n",
    "        \"\"\"Should call get_accuracy() after get_loss().\n",
    "        @return: return accuracy\n",
    "        \"\"\"\n",
    "        if self.wrapper_loss is not None:\n",
    "            return self.wrapper_loss.get_accuracy(targets)\n",
    "        else:\n",
    "            return self.loss.get_accuracy(targets)\n",
    "\n",
    "    @for_extract_embedding(maxChunk=10000, isMatrix=True)\n",
    "    def extract_embedding(self, inputs):\n",
    "        out1 = self.layer1(inputs)\n",
    "        out2 = self.layer2(out1) + out1\n",
    "        out3 = self.layer3(out1 + out2) + out1 + out2\n",
    "        out4 = self.layer4(out1 + out2 + out3) + out1 + out2 + out3\n",
    "        out = torch.cat([out2, out3, out4], dim=1)\n",
    "        out = self.bn_conv(F.relu(self.conv(out)))\n",
    "        x = self.bn_stats(self.stats(out))\n",
    "        if len(x.shape) !=3:\n",
    "            x = x.unsqueeze(dim=2)\n",
    "        if self.extracted_embedding == \"far\":\n",
    "            assert self.fc1 is not None\n",
    "            xvector = self.fc1.affine(x)\n",
    "        elif self.extracted_embedding == \"near_affine\":\n",
    "            x = self.auto(self.fc1, x)\n",
    "            xvector = self.fc2.affine(x)\n",
    "        elif self.extracted_embedding == \"near\":\n",
    "            x = self.auto(self.fc1, x)\n",
    "            xvector = self.fc2(x)\n",
    "        else:\n",
    "            raise TypeError(\"Expected far or near position, but got {}\".format(self.extracted_embedding))\n",
    "        return xvector\n",
    "\n",
    "    def get_warmR_T(T_0, T_mult, epoch):\n",
    "        n = int(math.log(max(0.05, (epoch / T_0 * (T_mult - 1) + 1)), T_mult))\n",
    "        T_cur = epoch - T_0 * (T_mult ** n - 1) / (T_mult - 1)\n",
    "        T_i = T_0 * T_mult ** (n)\n",
    "        return T_cur, T_i\n",
    "\n",
    "    def compute_decay_value(self, start, end, T_cur, T_i):\n",
    "        # Linear decay in every cycle time.\n",
    "        return start - (start - end)/(T_i-1) * (T_cur%T_i)\n",
    "\n",
    "    def step(self, epoch, this_iter, epoch_batchs):\n",
    "        # Heated up for t and s.\n",
    "        # Decay for margin and dropout p.\n",
    "        if self.use_step:\n",
    "            if self.step_params[\"m\"]:\n",
    "                current_postion = epoch*epoch_batchs + this_iter\n",
    "                lambda_factor = max(self.step_params[\"lambda_0\"],\n",
    "                                 self.step_params[\"lambda_b\"]*(1+self.step_params[\"gamma\"]*current_postion)**(-self.step_params[\"alpha\"]))\n",
    "                self.loss.step(lambda_factor)\n",
    "\n",
    "            if self.step_params[\"T\"] is not None and (self.step_params[\"t\"] or self.step_params[\"p\"]):\n",
    "                T_cur, T_i = get_warmR_T(*self.step_params[\"T\"], epoch)\n",
    "                T_cur = T_cur*epoch_batchs + this_iter\n",
    "                T_i = T_i * epoch_batchs\n",
    "\n",
    "            if self.step_params[\"t\"]:\n",
    "                self.loss.t = self.compute_decay_value(*self.step_params[\"t_tuple\"], T_cur, T_i)\n",
    "\n",
    "            if self.step_params[\"p\"]:\n",
    "                self.aug_dropout.p = self.compute_decay_value(*self.step_params[\"p_tuple\"], T_cur, T_i)\n",
    "\n",
    "            if self.step_params[\"s\"]:\n",
    "                self.loss.s = self.step_params[\"s_tuple\"][self.step_params[\"s_list\"][epoch]]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Input size: batch_size * seq_len * feat_dim\n",
    "    x = torch.zeros(2, 20, 300)\n",
    "    model = ECAPA_TDNN(inputs_dim=20,num_targets=5, channels=512, embd_dim=192)\n",
    "    out = model(x)\n",
    "    print(model)\n",
    "    print(out.shape)    # should be [2, 192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
